{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question: Task 1. The results must be listed by applying at least 2 different tokenizers and 2\n",
        "different lemmatizers representing the Tokenization and Lemmatization processes.**\n",
        "\n",
        "\n",
        "Youtube video: https://www.youtube.com/watch?v=sXreKUk9dmQ&t=71s"
      ],
      "metadata": {
        "id": "SFYiVE2SI-WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk # I use nltk library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlirt2V7HSX9",
        "outputId": "b10bbd18-f7db-4973-f219-4c45594c9e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "a62tAG0jL8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "yCUfHxIVHVq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "oisAbq4OJRnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt') # It is necessary for the NLTK library to perform tokenization effectively"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmIFvST9JfgL",
        "outputId": "2a8bdc3a-d518-45de-8eca-5084c5c8a337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"I found a very nice jacket. However, the price is $50, which is too much for me, so I decided not to buy it\"\n"
      ],
      "metadata": {
        "id": "4_yVBHu6Ismx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(s) # I use word_tokenize to see every single word\n",
        "sentences = sent_tokenize(s) # every single sentence"
      ],
      "metadata": {
        "id": "PvNnx0CmI5Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words # Pay attention to the fact that '$' and '50' are separate expressions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP1HTfYdJDbW",
        "outputId": "ecb1aa3a-eacf-439e-e5fb-7fa45a6babe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'found',\n",
              " 'a',\n",
              " 'very',\n",
              " 'nice',\n",
              " 'jacket',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'the',\n",
              " 'price',\n",
              " 'is',\n",
              " '$',\n",
              " '50',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'too',\n",
              " 'much',\n",
              " 'for',\n",
              " 'me',\n",
              " ',',\n",
              " 'so',\n",
              " 'I',\n",
              " 'decided',\n",
              " 'not',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rprXcaGeJi8k",
        "outputId": "c8a21c2e-8de6-4ff2-a531-81f93af3cf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I found a very nice jacket.',\n",
              " 'However, the price is $50, which is too much for me, so I decided not to buy it']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        " Lemmatization helps in obtaining the base or dictionary form of a word, which is useful for tasks like text analysis.\n"
      ],
      "metadata": {
        "id": "omgwRYfzLxtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize"
      ],
      "metadata": {
        "id": "M8GmziRzJky0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(s, r'\\w+|\\$[\\d\\.]+|\\S+', gaps=False) # I write regex to make some difference. Due to the regex, I can see the '$' and '50' expressions together."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlR_QY4mJrTv",
        "outputId": "82e26df5-0bf7-47b5-8bb1-a724871b9721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'found',\n",
              " 'a',\n",
              " 'very',\n",
              " 'nice',\n",
              " 'jacket',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'the',\n",
              " 'price',\n",
              " 'is',\n",
              " '$50',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'too',\n",
              " 'much',\n",
              " 'for',\n",
              " 'me',\n",
              " ',',\n",
              " 'so',\n",
              " 'I',\n",
              " 'decided',\n",
              " 'not',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() # I create an instance of the WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "iEM9jznt7st5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer # I import the WordNetLemmatizer from the nltk.stem module\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "words = [\"am\", \"is\", \"are\", \"was\", \"were\"]\n",
        "\n",
        "for w in words:\n",
        "    print(wnl.lemmatize(w, 'v'))\n",
        "\n",
        "# As a result, I can clearly see that the base form of all the 'words list' is 'be'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwNetZh279Bq",
        "outputId": "48703e37-1f1c-4390-db0f-615a604ccf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "be\n",
            "be\n",
            "be\n",
            "be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "G6twTXks3z-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "sm1QVPev4Lbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(\"going goes worse walks came\") # I process the text using spaCy"
      ],
      "metadata": {
        "id": "EHJnNn6u9D1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I iterate through each token in the processed document\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \"-\",token.lemma_)\n",
        "\n",
        "# All the base form of all words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT4DHmyz25O6",
        "outputId": "e44e0027-d1be-424f-f1fd-515f9bb54927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going - go\n",
            "goes - go\n",
            "worse - bad\n",
            "walks - walk\n",
            "came - come\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hB_wn07US5mO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}